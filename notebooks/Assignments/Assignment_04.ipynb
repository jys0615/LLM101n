{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbbb1cc93fe3e1c",
   "metadata": {},
   "source": [
    "# Assignment 4\n",
    "\n",
    "In this assignment, you will refactor the entire code to PyTorch, making it more modular and efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1e201da9092e12",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9bef55bdd3e6f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "from utils import load_text, set_seed, configure_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366f4a5e31b9d092",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a041bfe9980131",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MLPConfig:\n",
    "    root_dir: str = os.getcwd() + \"/../../\"\n",
    "    dataset_path: str = \"data/names.txt\"\n",
    "    device: torch.device = torch.device('cpu')  # Automatic device configuration\n",
    "\n",
    "    # Tokenizer\n",
    "    vocab_size: int = 0  # Set later\n",
    "    \n",
    "    # Model\n",
    "    context_size: int = 3\n",
    "    d_embed: int = 8\n",
    "    d_hidden: int = 64\n",
    "    \n",
    "    # Training\n",
    "    val_size: float = 0.1\n",
    "    batch_size: int = 32\n",
    "    max_steps: int = 6000  # Max of max_steps = 6421\n",
    "    lr: float = 0.01\n",
    "    val_interval: int = 100\n",
    "    log_interval: int = 100\n",
    "\n",
    "    seed: int = 101\n",
    "\n",
    "config = MLPConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37b1da5eb884f97",
   "metadata": {},
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aac9ba3a2c94cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 101\n"
     ]
    }
   ],
   "source": [
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8046aa4cb3a6469f",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec9748db8884490e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on mps\n"
     ]
    }
   ],
   "source": [
    "config.device = configure_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d6ad274a6fbbb",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9dbce085edefdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = [chr(i) for i in range(97, 123)]  # all alphabet characters\n",
    "chars.insert(0, \".\")  # Add special token\n",
    "config.vocab_size = len(chars)\n",
    "str2idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx2str = {idx: char for char, idx in str2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576b370aea4c1555",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cc4922d3b541a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text data from /Users/jung-yoonsuh/Desktop/KHUDA/LLM101n/notebooks/Assignments/../../data/names.txt (length: 228145 characters).\n"
     ]
    }
   ],
   "source": [
    "names = load_text(config.root_dir + config.dataset_path).splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bf63ac06c3b24c",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f91738376b5a8431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Val Split\n",
    "train_names, val_names = train_test_split(names, test_size=config.val_size, random_state=config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f27ec069c3321f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 28829\n",
      "Validation Size: 3204\n",
      "Train Example: keyler\n",
      "Validation Example: jessamae\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Size: {len(train_names)}\")\n",
    "print(f\"Validation Size: {len(val_names)}\")\n",
    "print(f\"Train Example: {train_names[0]}\")\n",
    "print(f\"Validation Example: {val_names[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44931589173ddd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(_names):\n",
    "    _inputs, _targets = [], []\n",
    "\n",
    "    for name in _names:\n",
    "        context = [0] * config.context_size\n",
    "\n",
    "        for char in name + \".\":\n",
    "            idx = str2idx[char]\n",
    "            _inputs.append(context)\n",
    "            _targets.append(idx)\n",
    "            context = context[1:] + [idx]  # Shift the context by 1 character\n",
    "\n",
    "    _inputs = torch.tensor(_inputs)\n",
    "    _targets = torch.tensor(_targets)\n",
    "\n",
    "    return _inputs, _targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27772dc3e0015a64",
   "metadata": {},
   "source": [
    "### Task 1: PyTorch DataLoader\n",
    "\n",
    "We have been using plain Python lists to and then converted them to PyTorch tensors. This is not efficient since it is loading the entire dataset into memory.\n",
    "\n",
    "PyTorch provides `Dataset` and `DataLoader` class to load the data in memory on the fly. [PyTorch Documentation](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "\n",
    "Refactor the `prepare_dataset` function into a PyTorch `Dataset` class and use the `DataLoader` to efficiently load the data in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b550956d3a003a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class NamesDataset(Dataset):\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # PyTorch Dataset requires 3 methods:                                          #\n",
    "    # __init__ method to initialize the dataset                                    #\n",
    "    # __len__ method to return the size of the dataset                             #\n",
    "    # __getitem__ method to return a sample from the dataset                       #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    def __init__(self, _names: List[str], context_size: int):\n",
    "        \"\"\"\n",
    "        Initialize the dataset\n",
    "        \n",
    "        Args:\n",
    "            _names (List[str]): List of names\n",
    "            context_size (int): Context size of the model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "        self.names = _names\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the number of samples in the dataset\n",
    "\n",
    "        Returns:\n",
    "            (int): Number of samples\n",
    "        \"\"\"\n",
    "        length = len(self.names)\n",
    "        return length\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Return a sample from the dataset\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Input and target tensors\n",
    "        \"\"\"\n",
    "        name = self.names[idx]\n",
    "        context = [0] * self.context_size\n",
    "        inputs, targets = [], []\n",
    "        \n",
    "        for char in name + \".\":\n",
    "            idx = str2idx[char]\n",
    "            inputs.append(context)\n",
    "            targets.append(idx)\n",
    "            context = context[1:] + [idx]\n",
    "        \n",
    "        input_ids = torch.tensor(inputs)\n",
    "        target_id = torch.tensor(targets)\n",
    "        \n",
    "        return input_ids, target_id\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "352be875ddc0fa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataset\n",
    "train_dataset = NamesDataset(train_names, config.context_size)\n",
    "val_dataset = NamesDataset(val_names, config.context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27812caaf7fd7ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Train Samples: 28829\n",
      "Number of Validation Samples: 3204\n",
      "First train (input, target): (tensor([[ 0,  0,  0],\n",
      "        [ 0,  0, 11],\n",
      "        [ 0, 11,  5],\n",
      "        [11,  5, 25],\n",
      "        [ 5, 25, 12],\n",
      "        [25, 12,  5],\n",
      "        [12,  5, 18]]), tensor([11,  5, 25, 12,  5, 18,  0]))\n",
      "First validation (input, target): (tensor([[ 0,  0,  0],\n",
      "        [ 0,  0, 10],\n",
      "        [ 0, 10,  5],\n",
      "        [10,  5, 19],\n",
      "        [ 5, 19, 19],\n",
      "        [19, 19,  1],\n",
      "        [19,  1, 13],\n",
      "        [ 1, 13,  1],\n",
      "        [13,  1,  5]]), tensor([10,  5, 19, 19,  1, 13,  1,  5,  0]))\n",
      "Second train (input, target): (tensor([[ 0,  0,  0],\n",
      "        [ 0,  0, 20],\n",
      "        [ 0, 20,  9],\n",
      "        [20,  9, 20],\n",
      "        [ 9, 20, 21],\n",
      "        [20, 21, 19]]), tensor([20,  9, 20, 21, 19,  0]))\n",
      "Second validation (input, target): (tensor([[ 0,  0,  0],\n",
      "        [ 0,  0,  8],\n",
      "        [ 0,  8,  1],\n",
      "        [ 8,  1, 25],\n",
      "        [ 1, 25,  7],\n",
      "        [25,  7,  5],\n",
      "        [ 7,  5, 14]]), tensor([ 8,  1, 25,  7,  5, 14,  0]))\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Train Samples: {len(train_dataset)}\")\n",
    "print(f\"Number of Validation Samples: {len(val_dataset)}\")\n",
    "print(f\"First train (input, target): {train_dataset[0]}\")\n",
    "print(f\"First validation (input, target): {val_dataset[0]}\")\n",
    "print(f\"Second train (input, target): {train_dataset[1]}\")\n",
    "print(f\"Second validation (input, target): {val_dataset[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9e04718940ac55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Initialize the DataLoader for the training and validation datasets.          #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13763508cb10bf45",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [5, 3] at entry 0 and [7, 3] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example batch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m _x, _y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_loader))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)   \u001b[38;5;66;03m# (batch_size, context_size)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_y\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# (batch_size)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM101n/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM101n/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM101n/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM101n/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[38;5;241m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM101n/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:212\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 212\u001b[0m         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM101n/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM101n/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(batch, \u001b[38;5;241m0\u001b[39m, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [5, 3] at entry 0 and [7, 3] at entry 1"
     ]
    }
   ],
   "source": [
    "# Example batch\n",
    "_x, _y = next(iter(train_loader))\n",
    "print(f\"Input Shape: {_x.shape}\")   # (batch_size, context_size)\n",
    "print(f\"Target Shape: {_y.shape}\")  # (batch_size)\n",
    "print(f\"Input: {_x[0]}\")\n",
    "print(f\"Target: {_y[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b0001776a8667d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3035259e2763490c",
   "metadata": {},
   "source": [
    "### Task 2: MLP Model\n",
    "\n",
    "Initialize the weights of the model using the `Kaiming` initialization.\n",
    "\n",
    "What are other activation functions that can be used instead of `tanh`? What are the advantages and disadvantages? Use different activation functions and compare the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fbb0e90bc505757",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, d_embed, d_hidden):\n",
    "        super().__init__()\n",
    "        self.C = nn.Parameter(torch.randn(vocab_size, d_embed))\n",
    "        self.W1 = nn.Parameter(torch.randn(context_size * d_embed, d_hidden))\n",
    "        self.b1 = nn.Parameter(torch.randn(d_hidden))\n",
    "        self.W2 = nn.Parameter(torch.randn(d_hidden, vocab_size))\n",
    "        self.b2 = nn.Parameter(torch.randn(vocab_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_embed = self.C[x]\n",
    "        x = x_embed.view(x.size(0), -1)\n",
    "        \n",
    "        h = F.relu(x @ self.W1 + self.b1)\n",
    "        \n",
    "        logits = torch.matmul(h, self.W2) + self.b2\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78374911f6424dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP()\n",
      "Number of parameters: 3571\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "mlp = MLP(config.vocab_size, config.context_size, config.d_embed, config.d_hidden)\n",
    "mlp.to(config.device) # Move the model to the device\n",
    "print(mlp)\n",
    "print(\"Number of parameters:\", sum(p.numel() for p in mlp.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d6bd917a11788c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4357b5eacdb2bb5a",
   "metadata": {},
   "source": [
    "### Task 3: Wandb Integration\n",
    "\n",
    "[Weights and Biases](https://wandb.ai/site) is a platform to track your machine learning experiments. It is very useful to log the hyperparameters, metrics, and weights of the model. (We can't use matplotlib every time to visualize the results)\n",
    "\n",
    "Create a free account on Wandb. Initialize the wandb run and log the hyperparameters and metrics.\n",
    "\n",
    "**How to set up WANDB API KEY**\n",
    "- Create an account on Wandb\n",
    "- Go to `wandb.ai` -> `Settings` -> `API Keys` -> `Copy API Key`\n",
    "- Set the API key as an environment variable `WANDB_API_KEY`\n",
    "    - What is an environment variable? How to set it? Google `.env`\n",
    "\n",
    "Note: Do not hardcode the API key in the script. Use environment variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c5c45d3303decfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/jung-yoonsuh/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myoonsuh0615\u001b[0m (\u001b[33myoonsuh0615-kyung-hee-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/jung-yoonsuh/Desktop/KHUDA/LLM101n/notebooks/Assignments/../../wandb/run-20250408_002029-me2tpxj1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yoonsuh0615-kyung-hee-university/Assignment-04/runs/me2tpxj1' target=\"_blank\">vague-shape-1</a></strong> to <a href='https://wandb.ai/yoonsuh0615-kyung-hee-university/Assignment-04' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yoonsuh0615-kyung-hee-university/Assignment-04' target=\"_blank\">https://wandb.ai/yoonsuh0615-kyung-hee-university/Assignment-04</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yoonsuh0615-kyung-hee-university/Assignment-04/runs/me2tpxj1' target=\"_blank\">https://wandb.ai/yoonsuh0615-kyung-hee-university/Assignment-04/runs/me2tpxj1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/yoonsuh0615-kyung-hee-university/Assignment-04/runs/me2tpxj1?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1603e2420>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=os.environ.get(\"aa3f6196bb8c5303ba66c251386d5495551ea13e\"))\n",
    "wandb.init(\n",
    "    project=\"Assignment-04\",\n",
    "    config={\n",
    "        \"d_embed\": config.d_embed,\n",
    "        \"d_hidden\": config.d_hidden,\n",
    "        \"lr\": config.lr,\n",
    "    },\n",
    "    dir=config.root_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a58f16a9e13134",
   "metadata": {},
   "source": [
    "### Task 4: Training\n",
    "\n",
    "Train the model. Change the hyperparameters and configurations. Log the results and analyze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0dd7f1c2c2b366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        max_steps: int,\n",
    "        lr: float,\n",
    "        val_interval: int,\n",
    "        log_interval: int,\n",
    "        device: torch.device,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model for a fixed number of steps.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        train_loader (DataLoader): DataLoader for the training data.\n",
    "        val_loader (DataLoader): DataLoader for the validation data.\n",
    "        max_steps (int): Maximum number of steps to train.\n",
    "        lr (float): Learning rate.\n",
    "        val_interval (int): Interval for validation.\n",
    "        log_interval (int): Interval for logging.\n",
    "        device (torch.device): Device to run the model on.\n",
    "    \"\"\"\n",
    "    wandb.watch(model, log=\"all\", log_freq=log_interval)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    progress_bar = tqdm(enumerate(train_loader, start=1), total=max_steps, desc=\"Training\")\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for step, (train_inputs, train_targets) in progress_bar:\n",
    "        if step > max_steps:\n",
    "            break\n",
    "        model.train()\n",
    "        train_inputs, train_targets = train_inputs.to(device), train_targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(train_inputs)\n",
    "        loss = F.cross_entropy(logits, train_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=f\"{running_loss / step:.4f}\")\n",
    "\n",
    "        if step % val_interval == 0:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            total_samples = 0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_targets in val_loader:\n",
    "                    val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "                    val_logits = model(val_inputs)\n",
    "                    batch_loss = F.cross_entropy(val_logits, val_targets)\n",
    "                    val_loss += batch_loss.item() * val_inputs.size(0)\n",
    "                    total_samples += val_inputs.size(0)\n",
    "            wandb.log({\"Val Loss\": val_loss / total_samples}, step=step)\n",
    "\n",
    "        if step % log_interval == 0:\n",
    "            wandb.log({\"Train Loss\": running_loss / step}, step=step)\n",
    "\n",
    "    progress_bar.close()\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd22ec935ba3f10",
   "metadata": {},
   "source": [
    "Note: Unfortunatley PyTorch does not support infinite DataLoader. The train will stop when it reaches the end of the DataLoader. (max_steps=6421)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efed6ca46233328d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/6000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [8, 3] at entry 0 and [6, 3] at entry 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmlp,\n\u001b[1;32m      3\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[1;32m      4\u001b[0m     val_loader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[1;32m      5\u001b[0m     max_steps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_steps,\n\u001b[1;32m      6\u001b[0m     lr\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlr,\n\u001b[1;32m      7\u001b[0m     val_interval\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mval_interval,\n\u001b[1;32m      8\u001b[0m     log_interval\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlog_interval,\n\u001b[1;32m      9\u001b[0m     device\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m     10\u001b[0m )\n",
      "Cell \u001b[0;32mIn[27], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, max_steps, lr, val_interval, log_interval, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_loader, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), total\u001b[38;5;241m=\u001b[39mmax_steps, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (train_inputs, train_targets) \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m>\u001b[39m max_steps:\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM101n/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM101n/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM101n/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM101n/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM101n/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[38;5;241m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM101n/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:212\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 212\u001b[0m         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM101n/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/LLM101n/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(batch, \u001b[38;5;241m0\u001b[39m, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [8, 3] at entry 0 and [6, 3] at entry 2"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=mlp,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    max_steps=config.max_steps,\n",
    "    lr=config.lr,\n",
    "    val_interval=config.val_interval,\n",
    "    log_interval=config.log_interval,\n",
    "    device=config.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b18f523de26ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Analyze the results                                                          #\n",
    "# What hyperparameters worked well? What activation did you use? etc.          #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "#\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0c069cfb0e2b6f",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b3ab05935f1d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name(model: nn.Module, context_size: int, decoder: dict, end_id: int, device: torch.device) -> str:\n",
    "    \"\"\"\n",
    "    Generate a name using the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to generate the name.\n",
    "        context_size (int): Context size of the model.\n",
    "        decoder (dict): Decoder dictionary to convert indices to characters.\n",
    "        end_id (int): End token id.\n",
    "        device (torch.device): Device to run the model on\n",
    "\n",
    "    Returns:\n",
    "        (str): Generated name\n",
    "    \"\"\"\n",
    "    new_name = []\n",
    "    context = [end_id] * context_size\n",
    "\n",
    "    while True:\n",
    "        x = torch.tensor(context).unsqueeze(0).to(device)\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx = torch.multinomial(probs, num_samples=1).item()\n",
    "        new_name.append(decoder[idx])\n",
    "        context = context[1:] + [idx]\n",
    "        if idx == end_id:\n",
    "            break\n",
    "\n",
    "    return \"\".join(new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77d393036879fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    print(generate_name(\n",
    "        model=mlp,\n",
    "        context_size=config.context_size,\n",
    "        decoder=idx2str,\n",
    "        end_id=str2idx[\".\"],\n",
    "        device=config.device\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
